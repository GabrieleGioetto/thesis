@article{xu2019understanding,
  title   = {Understanding and improving layer normalization},
  author  = {Xu, Jingjing and Sun, Xu and Zhang, Zhiyuan and Zhao, Guangxiang and Lin, Junyang},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {32},
  year    = {2019}
}

@article{ba2016layer,
  title   = {Layer normalization},
  author  = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal = {arXiv preprint arXiv:1607.06450},
  year    = {2016}
}

@misc{rope-eleutherai,
  title        = {Rotary Embeddings: A Relative Revolution},
  author       = {Biderman, Stella and Black, Sid and Foster, Charles and Gao, Leo and Hallahan, Eric and He, Horace and Wang, Ben and Wang, Phil},
  howpublished = {\url{blog.eleuther.ai/}},
  note         = {[Online; accessed ]},
  year         = {2021}
}

@misc{byte-pair-encoding,
  title        = {The Modern Tokenization Stack for NLP: Byte Pair Encoding},
  author       = {LucyTalks},
  howpublished = {\url{https://lucytalksdata.com/the-modern-tokenization-stack-for-nlp-byte-pair-encoding}},
  note         = {[Online; accessed ]},
  year         = {2022}
}


@misc{gptj,
  author       = {Wang, Ben and Komatsuzaki, Aran},
  title        = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
  year         = 2021,
  month        = May
}

@misc{sap,
  author       = {SAP},
  title        = {{SAP Security Research}},
  howpublished = {\url{https://www.sap.com/documents/2022/03/8c5580cc-207e-0010-bca6-c68f7e60039b.html}},
  note         = {[Online; accessed ]},
  year         = 2022
}

@article{gao2020pile,
  title   = {The pile: An 800gb dataset of diverse text for language modeling},
  author  = {Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal = {arXiv preprint arXiv:2101.00027},
  year    = {2020}
}

@article{bellovin2019privacy,
  title     = {Privacy and synthetic datasets},
  author    = {Bellovin, Steven M and Dutta, Preetam K and Reitinger, Nathan},
  journal   = {Stan. Tech. L. Rev.},
  volume    = {22},
  pages     = {1},
  year      = {2019},
  publisher = {HeinOnline}
}

@article{chen2017learning,
  title   = {Learning discrete Bayesian networks from continuous data},
  author  = {Chen, Yi-Chun and Wheeler, Tim A and Kochenderfer, Mykel J},
  journal = {Journal of Artificial Intelligence Research},
  volume  = {59},
  pages   = {103--132},
  year    = {2017}
}

@article{pilan2022text,
  title   = {The text anonymization benchmark (tab): A dedicated corpus and evaluation framework for text anonymization},
  author  = {Pil{\'a}n, Ildik{\'o} and Lison, Pierre and {\O}vrelid, Lilja and Papadopoulou, Anthi and S{\'a}nchez, David and Batet, Montserrat},
  journal = {arXiv preprint arXiv:2202.00443},
  year    = {2022}
}

@article{radford2019language,
  title   = {Language models are unsupervised multitask learners},
  author  = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal = {OpenAI blog},
  volume  = {1},
  number  = {8},
  pages   = {9},
  year    = {2019}
}

@article{gpt3,
  title   = {Language models are few-shot learners},
  author  = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal = {Advances in neural information processing systems},
  volume  = {33},
  pages   = {1877--1901},
  year    = {2020}
}

@inproceedings{pitler2008revisiting,
  title     = {Revisiting readability: A unified framework for predicting text quality},
  author    = {Pitler, Emily and Nenkova, Ani},
  booktitle = {Proceedings of the 2008 conference on empirical methods in natural language processing},
  pages     = {186--195},
  year      = {2008}
}

@article{crossley2011development,
  title     = {The development of writing proficiency as a function of grade level: A linguistic analysis},
  author    = {Crossley, Scott A and Weston, Jennifer L and McLain Sullivan, Susan T and McNamara, Danielle S},
  journal   = {Written Communication},
  volume    = {28},
  number    = {3},
  pages     = {282--311},
  year      = {2011},
  publisher = {Sage Publications Sage CA: Los Angeles, CA}
}

@article{mcnamara2010linguistic,
  title     = {Linguistic features of writing quality},
  author    = {McNamara, Danielle S and Crossley, Scott A and McCarthy, Philip M},
  journal   = {Written communication},
  volume    = {27},
  number    = {1},
  pages     = {57--86},
  year      = {2010},
  publisher = {Sage Publications Sage CA: Los Angeles, CA}
}

@article{grootendorst2022bertopic,
  title   = {BERTopic: Neural topic modeling with a class-based TF-IDF procedure},
  author  = {Grootendorst, Maarten},
  journal = {arXiv preprint arXiv:2203.05794},
  year    = {2022}
}

@article{zandie2021topical,
  title     = {Topical language generation using transformers},
  author    = {Zandie, Rohola and Mahoor, Mohammad H},
  journal   = {Natural Language Engineering},
  pages     = {1--23},
  year      = {2021},
  publisher = {Cambridge University Press}
}

@software{Faraglia_Faker,
  author  = {Faraglia, Daniele and {Other Contributors}},
  license = {MIT},
  title   = {{Faker}},
  url     = {https://github.com/joke2k/faker}
}

@article{radford2018improving,
  title     = {Improving language understanding by generative pre-training},
  author    = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year      = {2018},
  publisher = {OpenAI}
}

@article{brown2020language,
  title   = {Language models are few-shot learners},
  author  = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal = {Advances in neural information processing systems},
  volume  = {33},
  pages   = {1877--1901},
  year    = {2020}
}

@article{chowdhery2022palm,
  title   = {Palm: Scaling language modeling with pathways},
  author  = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal = {arXiv preprint arXiv:2204.02311},
  year    = {2022}
}

@article{hoffmann2022training,
  title   = {Training Compute-Optimal Large Language Models},
  author  = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal = {arXiv preprint arXiv:2203.15556},
  year    = {2022}
}

@article{scao2022bloom,
  title   = {BLOOM: A 176B-Parameter Open-Access Multilingual Language Model},
  author  = {Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  journal = {arXiv preprint arXiv:2211.05100},
  year    = {2022}
}

@article{su2021roformer,
  title   = {Roformer: Enhanced transformer with rotary position embedding},
  author  = {Su, Jianlin and Lu, Yu and Pan, Shengfeng and Wen, Bo and Liu, Yunfeng},
  journal = {arXiv preprint arXiv:2104.09864},
  year    = {2021}
}

@inproceedings{alammar-2021-ecco,
  title     = {Ecco: An Open Source Library for the Explainability of Transformer Language Models},
  author    = {Alammar, J},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations},
  year      = {2021},
  publisher = {Association for Computational Linguistics}
}

@inproceedings{shrikumar2017learning,
  title        = {Learning important features through propagating activation differences},
  author       = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
  booktitle    = {International conference on machine learning},
  pages        = {3145--3153},
  year         = {2017},
  organization = {PMLR}
}

@inproceedings{sundararajan2017axiomatic,
  title        = {Axiomatic attribution for deep networks},
  author       = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  booktitle    = {International conference on machine learning},
  pages        = {3319--3328},
  year         = {2017},
  organization = {PMLR}
}

@inproceedings{abid2021persistent,
  title     = {Persistent anti-muslim bias in large language models},
  author    = {Abid, Abubakar and Farooqi, Maheen and Zou, James},
  booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
  pages     = {298--306},
  year      = {2021}
}

@inproceedings{lucy-bamman-2021-gender,
  title     = {Gender and Representation Bias in {GPT}-3 Generated Stories},
  author    = {Lucy, Li  and
               Bamman, David},
  booktitle = {Proceedings of the Third Workshop on Narrative Understanding},
  month     = jun,
  year      = {2021},
  address   = {Virtual},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.nuse-1.5}
}

@article{weidinger2021ethical,
  title   = {Ethical and social risks of harm from language models},
  author  = {Weidinger, Laura and Mellor, John and Rauh, Maribeth and Griffin, Conor and Uesato, Jonathan and Huang, Po-Sen and Cheng, Myra and Glaese, Mia and Balle, Borja and Kasirzadeh, Atoosa and others},
  journal = {arXiv preprint arXiv:2112.04359},
  year    = {2021}
}
