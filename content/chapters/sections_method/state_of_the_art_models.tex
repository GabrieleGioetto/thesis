\section{State of The Art auto-regressive language model}
Autoregressive transformer-based Large Language Models are a sequence-to-sequence deep learning model that are pre-trained on a large corpus of data. They are designed to generate new tokens conditioning the model on some input text. The output probability distribution for the next token that the model generates is a probability distribution over all possible tokens in the corpus. \\
Autoregressive transformer-based Large Language Models have shown ability to solve many tasks\cite{gpt3}, varying from more classicalones like translation and question-answering to more peculiar one, like common sense reasoning and reading comprehension. \\
Mathematically, language models seek to maximize the likelihood of seeing some sentence by considering the product of conditional probabilities up to the point of generation
\begin{equation}
    p(w_T|\;\theta) = \prod_{i=1}^{T}p(w_i|\;w_{1,..,i-1}, \theta)
\end{equation}
Here we list some of the most popular auto-regressive language model:
\begin{itemize}
    \item \textbf{GPT-3}: GPT-3\cite{brown2020language} is an unsupervised AI language model developed by OpenAI. It is the successor to the previous iteration of GPT-2, and when released was the largest language model ever created. GPT-3 first showed that LLMs can be used for few-shot learning across different domains and can achieve state of the art results without  building a task-specific model.
    \item \textbf{PaLM}: Pathways Language Model\cite{chowdhery2022palm} is a a 540-billion parameter LM developed by Google. The model was trained through the use of Pathways, a new system which enables highly efficient training of very large neural networks across thousands of TPUs. PaLM few-shot evaluation can outperform or match the finetuned state of the art on a wide array of reasoning tasks.
    \item \textbf{Chinchilla}: Chinchilla\cite{hoffmann2022training} is a LM released by DeepMind. The peculiarity of Chinchilla is that it has 'only' 70B parameters, but outperforms models such as GPT-3 (175B). The authors of the paper demonstrate that for the same compute budget a smaller model trained on more data will perform better.
    \item \textbf{GPT-J}: GPT-J\cite{gptj} is an open-source alternative to OpenAI's GPT-3, the model was released by Eleuther AI, a group of independent researcher and has 6B parameters
    \item \textbf{BLOOM}: BLOOM\cite{scao2022bloom} is an open-source auto-regressive LM released by HuggingFace. BLOOM has a stronger focus on languages, differentiating itself from the majority of other LMs that mainly focus on the English language. In total, BLOOM was trained on 46 different languages
\end{itemize}
Amongst all of these LMs, we picked GPT-J, mainly because it was open-source, free and was not too big for our GPU memories. The other main alternative we have tried was GPT-2\cite{radford2019language}, the predecessor of GPT-3, which is now free-to-use. However, GPT-J's generated text was more coherent and more fluent \\

% TODO: add example of text generated by GPT-J and gpt 2


