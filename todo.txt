Spiegare nell introduzione SAP company
talk about gpt tokenizer

https://jalammar.github.io/illustrated-gpt2/


https://dugas.ch/artificial_curiosity/GPT_architecture.html

positional encoding: https://kazemnejad.com/blog/transformer_architecture_positional_encoding/

rotary embedding: https://blog.eleuther.ai/rotary-embeddings/

the intuition behind RoPE is that we can represent the token embeddings as complex numbers and their positions as pure rotations that we apply to them. If we shift both the query and key by the same amount, changing absolute position but not relative position, this will lead both representations to be additionally rotated in the same manner—as we will see in the derivation—thus the angle between them will remain unchanged and thus the dot product will also remain unchanged.
A response many of us at EleutherAI had when first coming across this was “how does this differ from sinusoidal embeddings,” so we feel it is worth discussing this comparison. There are two ways that rotary embeddings are different from sinusoidal embeddings:

Sinusoidal embeddings apply to each coordinate individually, while rotary embeddings mix pairs of coordinates
Sinusoidal embeddings add a  or  term, while rotary embeddings use a multiplicative factor.

difference between gpt-3 and gpt-j architecture:
* Rotary embedding for slightly better performance.
* Placing the attention layer and the feedforward layer in parallel for decreased communication.

TODO:
- ADD Byte Pair Encoding GPT V
- add ECCO visualization V
https://colab.research.google.com/github/jalammar/ecco/blob/main/notebooks/Ecco_Primary_Attributions.ipynb
https://jalammar.github.io/explaining-transformers/
make distinction between FNN part and activation part, maybe add two distinct plots
Images take from notebook saved on 
- Add most frequent words by category ( maybe wordcloud )
- Add ethical consideration part
- run topic modeling on created tickets and plot them instead of wordcloud maybe
    - https://github.com/MaartenGr/BERTopic#visualize-topics
- add image of bayesian network
- add in appendix all prompts additional info

Bayesian
[https://mbernste.github.io/files/notes/Psuedocounts.pdf]
https://cs.nyu.edu/~roweis/csc412-2004/notes/lec13x.pdf