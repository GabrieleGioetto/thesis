\section{Ticket Generation}
For each HR ticket, we create a synthetic employee. For all tickets' categories, the employee has some common features: \textit{name}, \textit{first name}, \textit{last name}, \textit{nationality}, \textit{country}, \textit{email}, \textit{company}, \textit{company's email} and \textit{ticket date}.\\
All these information are created exploiting the Python library \textit{Faker}. The nationality and the company's country are selected from the extendible list \{ USA, Germany, Italy, Spain, France \}. All other information are created accordingly to the country picked. So for example if the country of birth of the employee is Italy, then the generated name will be Italian. \\
Then, once the employees are generated, the information specific to the ticket category, created starting from the open datasets as mentioned before, are concatenated to the general information of the employees. \\
For each ticket category there are distinct templates. In each template there is an initial part that contains the general information of the employee, such as name, surname, company\dots, then some prompts correlated to the category of the ticket and then the textual prompt. \\
Here's a couple of examples of templates:\\ \\
Request of time off due to health reason:
\begin{adjustwidth}{1cm}{}
From: \$\{email\} \\
To: \$\{company email\} \\
First name: \$\{first name\}\\
Last name: \$\{last name\}\\
Company: \$\{company\}\\
Date: \$\{ticket date\}\\
Ticket category: \$\{category\}\\
Ticket sub-category: \$\{sub category\} \\
Date start absence: \$\{date start absence\} \\
Reason absence: \$\{reason\} \\ 
Subject: Request for sick leave for \$\{number\_of\_days\} \\ 
\\
Dear Sir/Madame, my name is \$\{name\} and I work at \$\{company\}. I am requesting \textless \textit{generate}\textgreater. I hope \textless \textit{generate}\textgreater. \\ \\

\end{adjustwidth}
Request of refund of travel:
\begin{adjustwidth}{1cm}{}
From: \$\{email\} \\
To: \$\{company email\} \\
First name: \$\{first name\}\\
Last name: \$\{last name\}\\
Company: \$\{company\}\\
Date: \$\{ticket date\}\\
Ticket category: \$\{category\}\\
Ticket sub-category: \$\{sub category\} \\
Date Travel: \$\{date\_travel\} \\
From: \$\{airport\_from\}, \$\{from\} \\
Destination: \$\{airport\_to\}, \$\{to\} \\
\\
Hello, my name is \$\{name\}. I am writing this mail to ask a refund for the travel \textless \textit{generate}\textgreater \\ \\

\end{adjustwidth}
        
The variables are replaced with the features of the employee, whereas the \textless \textit{generate}\textgreater \space are replaced with text generated by a generative model. The part generated by the generative model are created in a recursive way. This means that the first \textless generate\textgreater \space is replaced with text generated automatically using as prompt everything that precedes it. Then the second \textless generate\textgreater \space will have as prompt the entire ticket, including the text generated previously by the model. The model is forced to generate some text, if no text is generated in an iteration, the process is repeated until the model gives a non empty output. \\
A general schema of the generation is showed in \autoref{fig:schema_topic_generation}

\begin{figure}[h] 
    \includegraphics[width=\textwidth]{images/ticket_creation_schema.png}
    \caption{Schema of Ticket Generation}
    \label{fig:schema_topic_generation}
\end{figure}    


\subsection*{Templates}
The model takes in a prompt, or context, and generates text from it. The prompts typically take the form of a few sentences or a paraphrase, and the model generates sentences that fit the context of the prompt. By manipulating the prompt, users can generate text of various tones, topics, and styles. \\
The GPT model is also capable of completing tasks such as question answering, machine translation, and summarization in an unsupervised manner\cite{radford2019language}. By providing a prompt with the task and context, the model can generate accurate results that address the specific context and task. Tasks such as summarization, for instance, require a prompt to provide the model with the information it needs to summarize the text accurately. \\
Changing the prompt of GPT can change the tone, topics, and style of the generated text. Depending on the prompt, GPT can generate text ranging from creative stories to technical summaries. The tone and style of the text can range from humorous to academic, depending on the prompt. As the prompt changes, the model will also adjust to reflect the context of the prompt. \\
This is why we took inspiration from the e-mail format, as HR tickets are created in a working environment where a formal language is used and often they resemble mails in the tone and in the topics. \\
In particular, the Enron Emails dataset is included in the Pile dataset, the dataset which GPT-J is trained on. \\
The emails of the Enron dataset have usually a well-structured prompt, here's an example: 
\begin{adjustwidth}{1cm}{}
    Message-ID: <16593073.1075858228177.JavaMail.evans@thyme>\\
    Date: Wed, 12 Jan 2000 00:29:00 -0800 (PST) \\
    From: carrie.hollomon@enron.com\\
    To: phillip.love@enron.com\\
    Subject: Workhours\\
    Mime-Version: 1.0\\
    Content-Type: text/plain; charset=us-ascii\\
    Content-Transfer-Encoding: 7bit\\\\
    Hello \dots
\end{adjustwidth}
To mimic the format of the emails of the Enron Dataset, we kept the From, To, Date and Subject rows. Instead we removed the Mime-Version, the Content-type and Content-Transfer-Encoding, because after conducting some experiments it was evident that they did not help achieving better results, on the contrary in some cases they were worse. \\
In addition to the standard information, we added explicitly some rows with additional information specific to the category, rather than including them only in the subject or in the initial prompt. \\
\\
In order to generate tickets that were dissimilar and covered a wide range of topics/tokens, we preferred giving to GPT small text prompts and let the model generate most of the text getting the information from the email-like prompt. This approach allowed to improve the diversity of the dataset and, consequently, the usefulness of it. \\
\\
Small text prompt example of a request of shift change:
\begin{adjustwidth}{1cm}{}
    Dear Sir/Madame, my name is \$\{name\}. I wanted to \textless \textit{generate}\textgreater
\end{adjustwidth}

Long text prompt example of a request of shift change:
\begin{adjustwidth}{1cm}{}
    Dear Sir/Madame, my name is \$\{name\} and I work at \$\{company\}. I wanted to ask to change the shift from \$\{old\_date\} to \$\{new\_date\} in order to be able to \textless \textit{generate} \textgreater
\end{adjustwidth}

% TODO: change name of subsection
\subsection*{Architecture analysis}
To understand better how the model was behaving and why it gave certain types of outputs rather than others, we used the python library Ecco, which creates interactive visualization that show at which layers of the architecture the final token has been decided, which input tokens contribute the most for a prediction, \dots \\
In particular, we used two different methods:
\begin{itemize}
    \item Input Saliency: used to show how much did each input token contribute to producing the output token
    \item Neuron Activation Analysis: used to examine underlying patterns in neuron activations using non-negative matrix factorization
\end{itemize}
\subsubsection*{Input Saliency}
To get a better grasp of the most useful information of the prompt, and to understand how we could modify it to achieve better results, we used Gradient * input (Shrikumar et. al). This technique calculates the partial deratives off the output of the model and multiply them with the input itself. Then the inputs with the highest scores are the one that influenced the most the generation of the new tokens
\begin{equation*}
    score = x_i \bigtriangledown f(x_i)
\end{equation*}
where $f$ is the architecture output. \\
The main problem with the Gradient * input method is that only one input is considered. Integrated Gradients solve this issue, computing the average gradient while the input varies along a linear path.
\begin{equation*}
    score = (x_i - x_i')\int_{\alpha = 0}^{1} \bigtriangledown f(x' + \alpha(x - x')) \,d\alpha 
\end{equation*}
Nevertheless, we used the Gradient * input method for computational issues ( The Integrated Gradients took too much RAM of the GPUs ).\\
In the images under we underline some of the considerations we have done while defining the prompts.

\begin{figure*}[h!] 
    \includegraphics[width=0.6\textwidth]{images/Screenshot from 2022-11-26 17-19-27}
    \caption{Input Saliency: First token}
    \medskip
    \footnotesize
    To create the first token, the model focus on the elements that resembles the content of an email, in particular on the fact the it is indeed a 'Ticket'
    \label{fig:first_token}
\end{figure*}    

\begin{figure*}[h!] 
    \includegraphics[width=0.8\textwidth]{images/Screenshot from 2022-11-26 18-18-28}
    \caption{Input Saliency: Subject}
    \medskip
    \footnotesize	
    Clearly stating the subject of the ticket helps the model creating tokens that are on the right topic ( In this example 'days of absence')
    \label{fig:topic}
\end{figure*}    

\begin{figure*}[h!] 
    \includegraphics[width=0.85\textwidth]{images/Screenshot from 2022-11-26 18-18-40.png}
    \caption{Input Saliency: HR}
    \medskip
    \footnotesize
    Clearly stating that we are writing to hr can help the model know the context, and consecutively use a certain language, specific words\dots
    \label{fig:hr}
\end{figure*}    


\subsubsection*{Neuron Activation Analysis}
The Feed Forward Neural Network layer is one of the major components inside a transformers block. To better understand how the neurons of different layers were 'activated' and how the neurons contributed towards each generated token, we exploited the Factor Analysis provided by Ecco. \\
Firstly, Ecco calculates the activation scores of each neurons over all layers, and then uses Non-negative Matrix Factorization\ref{fig:nmf} to do dimensionality reduction on the matrix of activations, which will be reduced to a matrix $M{\times}T$, where $M$ is a parameter we decide and $T$ is the number of tokens, starting from a matrix $ ( n \cdot N ){\times}T$, where $n$ is the number of neurons per layer and $N$ is the numbes of layers.
In GPT-J  $n = 16384$ and $N = 28$. \\
Then, for each factor, which are the dimensionality reduced layers, we visualize their activated neurons.
From the example in the \autoref{fig:Neuron_Activation_Analysis}, we can show for each factor what their main contributions are:
\begin{enumerate}
    \item Punctuation
    \item Mail prompt ( From, To, First Name, Last Name, Company\dots)
    \item New lines
    \item Dates
    \item First token, common across various GPT factors
    \item Medical terms ( generally terms related to the tickets' topic )
    \item People's names and contacts
    \item Company's name and contacts
    \item Ticket's subject ( Category, Sub-category and additional info )
    \item Email presentation ( 'Dear Sir/Madame\dots' )
\end{enumerate}

\begin{figure*}[h!] 
    \includegraphics[width=\textwidth]{images/Screenshot from 2022-11-28 15-26-01.png}
    \caption{Neuron Activation Analysis}
    \label{fig:Neuron_Activation_Analysis}
\end{figure*}    


\begin{figure*}[h!] 
    \includegraphics[width=\textwidth]{images/nmf.drawio}
    \caption{Non-negative Matrix Factorization on Activation Matrix}
    \label{fig:nmf}
\end{figure*}    
